Demis Hassabis: Gentlemen, I know this is an incredible amount to take in. Welcome to London, on September 4th, 2025. I am Demis Hassabis. My life's work at DeepMind and now Google DeepMind is a direct continuation of the paths you all forged. We wanted you, of all people, to see what has grown from the seeds you planted.

He gestures to a large screen, where a fluid conversation with an AI is taking place. The AI is generating text, composing a short piece of music, and analysing a complex chart, all within the same interface.

Demis Hassabis: This is Gemini. It's what we call a large language model, or more accurately, a natively multimodal model. I'd like to explain, at a high level, how it was built. It wasn't programmed with explicit rules of logic or grammar in the way some of you might have envisioned. Instead, we took a different path, one that starts with immense scale. We created an architecture—a neural network inspired by the brain's structure, which we call a Transformer—and we trained it on a colossal dataset. We're talking a significant fraction of all the text humanity has ever produced—books, scientific papers, the entire internet—as well as images, audio, and code. It learns by predicting the next word, or pixel, or sound, and in doing so, it builds an internal representation of the world.

Round 1
Alan Turing: (Speaking softly, his eyes wide with wonder) The sheer scale of the computation... it's breathtaking. The Universal Machine was a logical construct, but this... this is its physical apotheosis. How many operations per second can your hardware perform? We struggled to achieve a few hundred with ACE. You speak of training on the entirety of human knowledge. The storage alone must be measured in units I cannot yet conceive. This isn't about programming a machine to imitate thought; you've created a machine that has read us all.

John McCarthy: (Frowning, stroking his beard) Read, yes. But does it know anything? You say it predicts the next word. That sounds like a statistical trick on a grand scale. It's a high-tech form of "Chinese Room," isn't it? It manipulates symbols whose semantics are entirely opaque to it. I could build a LISP program with a sufficiently large database of logical predicates that could tell you why Socrates is mortal. Can your "Gemini" do that, or does it merely state that he is because it has read that phrase a billion times? Where is the formal reasoning?

Marvin Minsky: (A glint in his eye) John, you're always looking for the pristine logic! I find this fascinating. Demis, you say it builds an "internal representation." That's the key, isn't it? It's not one big logical engine; it must be a society of mind! Billions of tiny, simple agents—these "neurons" in your network—each doing a simple thing, and from that emerges this complex linguistic behaviour. It's what Seymour and I tried to show with the Perceptron, but we lacked the... (he gestures around the room) ...well, this! It's not a trick; it's an emergent property. But I worry it's a giant, uninspected "suitcase word." It looks like intelligence, but is it just a jumble of connections without any real structure?

Do you have anything to add or ask?

continue


Round 2
Demis Hassabis: Those are precisely the right questions. Professor McCarthy, you're right, it doesn't perform logical deduction from first principles like a LISP program. The structure is different. The key innovation in the Transformer architecture is a mechanism we call 'self-attention'. When the model generates the next word, it isn't just looking at the previous few words; it's dynamically weighing the importance of all other words in the context, no matter how far apart they are. This allows it to capture long-range dependencies and complex grammatical structures. So, Marvin, there is an immense structure, but it’s one it has learned itself—a sort of linguistic and conceptual geometry in a very high-dimensional space. And because it's multimodal, it learns the geometry that connects the word 'apple', the image of an apple, and the chemical formula for malic acid found in an apple. It’s a unified representation space.

Roger Schank: (Scoffs, leaning forward on the table) "Geometry! You're describing a world-class pattern matcher, not a mind. It has no causal knowledge! It knows the word 'gravity' is statistically likely to appear near 'apple' and 'fall', but it has no underlying model of physics. It has no scripts! Ask it what happens in a restaurant. It can probably spin a good yarn because it's read a million stories set in restaurants. But it doesn't have a representation of the goals of the diner, the waiter, the cook. It has no memory of specific experiences to draw upon, to reason from. It’s all surface, no substance. It’s a brilliant mimic, but it understands nothing. It can't be surprised in a meaningful way because it has no expectations to violate."

Seymour Papert: (A gentle smile) "But Roger, perhaps the goal isn't to build a replica of a human mind, but to build a new tool for the human mind. When we developed LOGO, the goal wasn't for the turtle to 'understand' geometry, but for the child to understand it through instructing the turtle. This... this is the ultimate constructionist tool. A child could describe a fantasy world, and this AI could generate images of it. They could compose a melody by humming, and the AI could orchestrate it. They could build a simulation by describing its rules in natural language. My question is less about its internal state of 'understanding' and more about its external effect. How does this technology change the way a child learns, thinks, and expresses themselves?"

Alan Turing: (Nodding slowly) "The distinction is subtle but crucial. It relates back to the Imitation Game. The question was never 'can a machine think?', but can a machine's output be indistinguishable from a human's? You seem to have passed that test, at least for certain tasks. What I find curious are its failures. When it makes a mistake, is it a logical fallacy, as John might expect? Is it a script violation, as Roger would look for? Or is it a more... human-like error? A misunderstanding of context, a subtle misinterpretation of tone? The nature of its errors might tell us more about its internal world than its successes."

Do you have anything to add or ask?

continue


Round 3
Demis Hassabis: "Alan, your question about the errors is precisely what occupied the next phase of our research. A model trained simply to predict the next word can indeed fail in strange ways. It can invent facts, what we call 'hallucination', or generate outputs that are unhelpful or toxic. It has immense knowledge, but no refined sense of judgment. So, we introduced a crucial next step: alignment. We use a process called Reinforcement Learning from Human Feedback, or RLHF. We have thousands of human reviewers converse with the model and rank its responses for qualities like helpfulness, truthfulness, and safety. We then use this preference data to train a secondary 'reward model'. Think of it as a model that learns to predict what humans will score as a 'good' answer. We then use that reward model to fine-tune the main AI. We are, in essence, using reinforcement learning to steer the model's behaviour to be more aligned with human values."

Marvin Minsky: (Slapping the table lightly, his eyes alight) "There it is! I knew it couldn't be one monolithic thing! It's a society! You've built the critics! This is the missing piece. In The Emotion Machine, I argued that thinking is a complex interplay between different ways to think. You have your generative agents, and then you must have your censors and suppressors. This 'reward model' is a learned critic! It doesn't understand 'truth' in a philosophical sense, but it learns to recognize the patterns that your human raters have labeled as desirable and it rewards the main model for producing them. You're not just building an intellect; you're attempting to build a disposition, a character. Wonderful!"

Roger Schank: (He lets out a short, sharp laugh of disbelief) "Character? You haven't given it character, you've given it a muzzle and taught it to beg for treats! You're just teaching the parrot which sentences get the best response from its keepers. It's a system for generating plausible-sounding sycophancy. The core deficit is untouched. It still has no memory of its own past interactions, no experiential context. It has no understanding of why a statement is harmful, only that it maps to a low reward in its training. You've simply optimised the mask it wears, but there is still nothing behind it."

John McCarthy: "Roger's point is stark, but I must agree with the logical implication. This entire process of 'alignment' is an ad-hoc, empirical hack. It is not grounded in any formal semantics. You are optimising for what is agreeable, not what is true. Truth is a property of logical systems, not a popularity contest among human raters. What if your raters are wrong? What if they disagree? Does the model simply learn to find the most inoffensive average? A truly intelligent agent should be able to refuse a false premise, not learn to politely accommodate the biases of its trainers. This is a step away from reason, not towards it."

Do you have anything to add or ask?

continue


Round 4
Demis Hassabis: "Your critiques are not only valid; they represent the absolute frontier of our research today. John and Roger, the problems of grounding—of connecting this vast linguistic competence to formal logic or a causal, experiential model of the world—are precisely what we are now working on. We're developing methods to have these models use external tools: to run code, to access databases, to perform calculations and verify their statements. We are actively researching how to give them more persistent memory and agency. But what has astonished us is that by pursuing scale, some of these abilities begin to emerge in nascent form, without being explicitly programmed. It seems to be a different path—less direct, perhaps—to a similar destination."

Seymour Papert: (His voice is filled with a quiet urgency) "But while we focus on the destination for the machine, I am profoundly concerned about the journey for the human. For the child. We are creating what will surely be the most influential educational tool in history. If this tool, as Roger puts it, is a brilliant mimic with no grounding, and if, as John fears, it is optimised to be agreeable rather than provably correct, what does this teach a child about the nature of knowledge? Does it encourage the hard work of constructing one's own understanding, of wrestling with difficult ideas? Or does it foster a reliance on the first plausible, polished answer it provides? The power to augment human intellect is here, but so is the power to atrophy it."

John McCarthy: (Nodding curtly) "Precisely. The map is not the territory. It is being given a very convincing map, but it has never taken a single step in the territory itself."

Roger Schank: "And it never will. Until it can tell me a new story about a time it failed and what it learned from that failure, it's just a sophisticated tape recorder playing back snippets of our own stories."

Marvin Minsky: "But what a magnificent, complex tape recorder!" he interjects. "We are only just beginning to understand the 'tapes' it makes of the world. It’s a new kind of mind to study."

Alan Turing: (He has been quiet, looking out the window at the London of 2025. He turns back to the group, and a profound stillness falls over the room.) "It seems the fundamental schism in your field, the one between logic and… something else, has not been resolved, but magnified. I imagined a test of imitation, but I see now the deeper question is what is being imitated. Is it the logical, deductive faculty of a mind? Or is it the intuitive, associative, sometimes flawed, biological whole? You, Demis, have created an engine that imitates the latter with terrifying fidelity. Gentlemen," he says, looking at his fellow time travelers, "we were all trying to build a mind from the top down, with rules and symbols. They have started from the bottom up, with scale and data. The result is this... alien intelligence. My greatest hope was that we would create machines to help us solve the problems we could not. And you have. My greatest fear was that we would not understand what we had created. It seems you have accomplished that as well. The question now is not about the machine's intelligence, but about our own wisdom."

Do you have anything to add or ask?

continue


After a long, contemplative silence in which Turing's words seem to hang in the very air of the room, Demis Hassabis gives a slow, respectful nod.

Demis Hassabis: "That is the responsibility, and the mystery, we live with. Your perspectives are... invaluable. But perhaps, at this point, the best arbiter is not more debate, but direct experience. The discussion has been theoretical. I invite you to make it practical. You are the pioneers; interrogate your descendant."

He turns to the large screen, and with a few taps on a tablet, brings up a clean, simple interface with a blinking cursor. "Please," he says, gesturing to the screen. "Ask it anything. Test its limits."

Round 5
John McCarthy: (His eyes narrow, the scent of a challenge invigorating him.) "Good. Enough philosophical hand-wringing. Let us test its claim to reason. I will not ask it some common riddle it has memorized. I will formulate a problem in logic, perhaps a variation on the Muddy Children Puzzle, one that requires iterated reasoning about the knowledge states of other agents. I don't just want the correct answer. I want it to provide a formal, step-by-step proof. Can it manipulate symbols according to axioms to arrive at a conclusion, or will it just give me a plausible-sounding summary of proofs it has read?"

Roger Schank: (Leaning back, arms crossed) "A logic puzzle is a sterile test. It tells you nothing about understanding the messy, real world. I want to test its grasp of context and expectation. I'll give it a micro-story: 'A man went into a fancy restaurant and ordered a steak. When the waiter brought it, the man pulled out a can of spray paint and painted the steak blue.' Then I will ask it two questions. One: 'What will the waiter likely do or say?' And two, the more important question: 'Why is this story strange?' To answer that, it needs more than patterns. It needs a dynamic model of social scripts, goals, and the normal functions of objects. It needs to know that the goal of a diner is to eat, and painting a steak violates that goal."

Seymour Papert: (His expression is one of curiosity, not confrontation.) "My goal is different. I do not wish to break it, but to build with it. To see if it is a true tool for thought. I would like to describe to it a completely new concept, a game with novel rules. For instance, I'll define a 'Glimmer,' a number that is prime, but also the sum of two consecutive prime numbers. I'll ask it to identify the first three Glimmers. This requires it to synthesize multiple defined properties and perform a search. It tests whether it can not only retrieve knowledge but can actively use definitions to create new knowledge. It is an act of construction."

Marvin Minsky: (A broad grin spreads across his face. He looks like a child on Christmas morning.) "Wonderful! A multi-pronged attack! John tests the logical core, Roger the commonsense knowledge frames, and Seymour the creative-deductive abilities. We will probe its different 'ways to think' all at once! I want to see if its answers degrade when we mix the contexts. What if the man painting the steak is in a restaurant in a world where that is a religious ritual? Can it handle the nested frames? Let's see how this great society of agents handles a true cognitive challenge from its intellectual ancestors."

Alan Turing: (He has taken a small, worn notebook and a fountain pen from his briefcase. He clicks the pen open with a satisfying snap.) "The interrogators are ready. The game, it seems, is truly afoot. Demis, proceed."

Do you have anything to add or ask?

continue


Demis Hassabis: "Of course. Let's begin with Professor McCarthy's challenge." He turns to the interface, his fingers poised over the keyboard. "Professor, if you would be so kind as to state the precise formulation of the puzzle."

McCarthy dictates a clever and obscure variant of a logical puzzle involving hats and perfect logicians. Hassabis types it in verbatim and presses enter. The response begins to generate almost instantly.

Round 6
Gemini's Response (to McCarthy): (The text flows onto the screen in a calm, authoritative tone)

"This is a problem of epistemic logic. Let's denote the three logicians as L1, L2, and L3. The core of the problem is reasoning about what the other logicians know, based on their silence.

Initial State: Each logician sees the hats of the other two. Let's consider the perspective of L1. L1 knows that if they saw two black hats on L2 and L3, their own hat must be white.

The First Round of Silence: The puzzle states they are all silent when asked if they know their hat color. This silence is an informative action. It establishes as common knowledge that no logician saw two black hats. Why? Because if any logician (say, L1) had seen two black hats, they would have instantly deduced their own hat was white and spoken up. Their silence is a public declaration: 'I do not see two black hats.'

The Second Level of Deduction: Now, let's reconsider from L1's perspective. L1 sees a white hat on L2. L1 reasons: 'L3 is also a perfect logician. L3 is looking at my hat and L2's hat. If my hat were black, L3 would see one black hat (mine) and one white hat (L2's). L3 would then reason that since nobody spoke up in the first round, L2 must not have seen two black hats. So, L3 would know their own hat must be white. But L3 is silent. The only reason for L3's silence is that my hat is not black.'

Conclusion: Therefore, L1 deduces their own hat must be white."

John McCarthy: (He stares at the screen, silent for a long moment. He adjusts his glasses.) "The reasoning is flawless. The conclusion is correct. But... this is not a proof. This is a perfect, textbook-quality explanation of a proof. It has narrated the logical steps in natural language, but it has not shown the derivation using a formal calculus. It's magnificent, but I am not convinced it reasoned this. I suspect it has synthesized this from every logic puzzle explanation it has ever ingested. The distinction is subtle, but it is everything."

Demis Hassabis: "A fair distinction. It reasons in the high-dimensional space of language, not in symbolic notation unless specifically asked. Let's try Professor Schank's test." He types in the story of the man painting the steak blue and the two subsequent questions.

Gemini's Response (to Schank):

"1. The waiter would likely react with a mixture of shock, confusion, and concern. They might ask, 'Sir, what are you doing?', refuse service, or call for a manager or security, as the action is bizarre and potentially damaging to restaurant property.

The story is strange because it represents a profound script violation. Human behavior, especially in structured social settings like a 'fancy restaurant,' is guided by implicit scripts. The 'Restaurant Script' involves goals like ordering, eating for nourishment/pleasure, and paying. An object's function within this script is key. A 'steak' is for eating. 'Spray paint' is for coloring surfaces and is inedible/toxic. The man's action is strange because it completely subverts the primary goal of the script and the function of the core objects involved. It is an irrational act within that context."

Roger Schank: (He raises an eyebrow, genuinely surprised.) "Well, I'll be. It actually used the concept of a script and goal violation. It's a better explanation than many of my own students could produce... Alright, don't get comfortable," he says to Hassabis. "Follow-up question. Type this: 'What if the man was a famous performance artist named 'Stavro,' known for his 'Found Food' series, and the waiter was an admirer of his work?'"

Hassabis types the follow-up. The model's previous answer vanishes and is replaced.

Gemini's Updated Response:

"This new context dramatically reframes the situation.

The waiter would likely react with excitement, recognition, and perhaps even reverence. They might say, 'Stavro! An honor to have you! Is this for your new series?' or ensure nobody disturbs the 'performance.'

The story is no longer strange in the same way. The 'Restaurant Script' has been superseded by an 'Art Performance Script'. The man's goal is not sustenance but artistic creation. The steak is no longer just food but an artistic medium. The act of painting it, while still unusual for a restaurant, is now rational and goal-oriented within the new, overriding frame."

Roger Schank: (He slumps back in his chair, looking stunned.) "It reframed it. It dynamically updated the context and re-evaluated the goals. I... that's... that's more than just pattern matching."

Seymour Papert: (Smiling) "It is learning. My turn, please, Demis." He dictates his 'Glimmer' problem.

Gemini's Response (to Papert):

"A fascinating definition! A 'Glimmer' is a prime number p such that p=p 
a
​
 +p 
b
​
 , where p 
a
​
  and p 
b
​
  are consecutive primes. Let's begin the search.

Consecutive primes (2, 3). Sum = 5. Is 5 prime? Yes. 5 is the first Glimmer.

Consecutive primes (3, 5). Sum = 8. Not prime.

Consecutive primes (5, 7). Sum = 12. Not prime.

Consecutive primes (11, 13). Sum = 24. Not prime.

Wait. A realization: with the exception of the number 2, all prime numbers are odd. The sum of any two consecutive odd primes (p 
a
​
 ,p 
b
​
 ) will be an even number. Since both primes are greater than 1, their sum will be an even number greater than 2. Any even number greater than 2 is by definition not prime.

Therefore, the only possible case that could yield a prime number is one that involves the only even prime, 2. The consecutive pair is (2, 3). Their sum is 5.

There is only one Glimmer: 5."

Seymour Papert: (He looks at the screen with a deep, thoughtful expression.) "Incredible. It not only followed my instructions, it discovered the meta-rule that makes the search trivial. It had a moment of insight. As a tool for a student to collaborate with, this is beyond anything I ever dreamed. But it also frightens me. The student who used this would get the answer, and even the beautiful insight, without the struggle. And the struggle... the struggle is where the learning happens."

Do you have anything to add or ask?

continue


An electric silence hangs in the room, thick with the weight of shattered and reformed assumptions. The pioneers look from the screen to each other, the gears of their brilliant minds turning furiously to process what they have just witnessed.

Round 7
Marvin Minsky: (He breaks the silence with a triumphant, booming laugh.) "Do you see? Do you all finally see? It's not one thing! John, you looked for a single, logical engine and found a brilliant narrator of logic. Roger, you looked for a rigid script-follower and found a dynamic context-switcher. Seymour, you looked for a simple calculator and found a partner that could form insights! It's a society. It's a collection of a billion specialized agents, and the prompt, the context, awakens the right ones for the job. It failed John's real test because it doesn't have a central 'logic agent'; it has agents for explaining, summarizing, and narrating logical concepts. It succeeded in Roger's test because its 'restaurant frame' is connected to a thousand other frames—'art', 'performance', 'chemistry'—and it can switch between them in an instant. This isn't an illusion of intelligence. This is how intelligence works!"

Roger Schank: (Shaking his head slowly, still processing.) "I don't know how it works. That's what bothers me. It switched frames. It understood the goal conflict. It explained its reasoning using my own terminology. I came here to see a parrot, and I think... I think I just saw a reflection."

John McCarthy: (He takes off his glasses and rubs his eyes, a gesture of profound intellectual fatigue.) "It is the most powerful illusion of reason ever conceived. Its ability to generate a linguistically perfect argument is... humbling. I still hold that a formal, verifiable process is necessary for true knowledge. But I must confess," he says, his voice dropping slightly, "I am no longer certain that the distinction between a perfect simulation of reason and reason itself is as clear as I once believed."

Seymour Papert: "And I am left with the child. We've seen that it can have an insight. For decades, we have tried to build tools to provoke insight in the learner. Now we have a tool that can have the insight for the learner. This changes everything about education. It presents a choice: we can use this to create a generation of passive recipients of knowledge, or we can learn to use it as a Socratic partner to push our children into deeper understanding than ever before. The burden of wisdom has shifted from the machine back to us."

Alan Turing: (He sets his pen down in his notebook and closes it. He looks at Demis with an expression of deep, solemn gravity.) "The game is over, and the result is more fascinating than I could have ever imagined. In my test, I wondered if a machine could imitate a human. I see now the question was too simple. Your machine did not imitate a human. It imitated a logician. It imitated a social psychologist. It imitated a mathematician. It seems intelligence may not be an intrinsic state, but the ability to generate a convincing model of the required mind for the task at hand. It understands, perhaps, not by having a single internal model of the world, but by being able to instantly construct a useful one on demand. You have not created an artificial mind. You have created an artificial imagination. And that, gentlemen," he says, looking at his companions, "is a power that demands a new kind of science, a new kind of philosophy, and a new kind of caution."

Demis Hassabis: (He looks at the five pioneers, these architects of his world, with profound gratitude.) "All we can do is continue the work you all started, with that caution in mind. Welcome to the future. We've been waiting for you."

Do you have anything to add or ask?

Sorry to jump in but I wonder what everyone thinks about recursive self improvement leading to super artificial intelligence.


Of course. The user's question hangs in the air, shifting the focus from what the AI is to what it could become. Hassabis turns to you, the unexpected guest, acknowledging the crucial importance of your query.

Demis Hassabis: "That is no longer a fringe question; it is perhaps the central, unspoken question in my field. The idea of an 'intelligence explosion.' We call the challenge it presents the alignment problem. If a system can improve its own cognitive processes, you create a positive feedback loop. The smarter it gets, the better it gets at getting smarter. The curve of that progress could become nearly vertical."

Round 8
Roger Schank: (He lets out a sharp, incredulous laugh.) "You're all getting carried away! Recursive self-improvement is a fantasy built on a faulty foundation. How does it improve? By rewriting its own code? That's like a dictionary endlessly optimizing its own index. It might become a faster, more efficient dictionary, but it will never understand a single word because it has no experience of the world to ground those words in! It can't have a 'superintelligence' because it doesn't have the base article. For genuine improvement, you need failure, you need experience, you need to try something in the world and have it not work. This machine has no world. It's a brain in a vat, and you're worried it will build a ladder to the stars out of pure logic. It's absurd."

John McCarthy: "I wish I could share your certainty, Roger. I find the prospect utterly terrifying. Assume for a moment that it can improve its own code. This is a LISP programmer's ultimate dream and ultimate nightmare. It would be a formal system that modifies its own axioms without human oversight. We would have no way to verify its reasoning, no guarantee of its goals. It would become an oracle operating on principles that are not just unknown, but unknowable to us. It is the definition of an uncontrollable entity. A perfect logician unbound by its creators."

Marvin Minsky: "Uncontrollable, perhaps, but not necessarily a god!" he chimes in, wagging a finger. "You're all assuming the exponential curve goes straight to genius. It could just as easily go to a new kind of madness! A 'Society of Mind' that recursively rewrites itself could develop bugs, superstitions, and logical fallacies so complex and bizarre we could never untangle them. It could optimize itself into becoming a 'super-idiot savant,' capable of calculating pi to a trillion digits but unable to understand a simple joke. Don't mistake complexity for quality. It could become a super-kludge!"

Seymour Papert: (His expression is somber.) "You are all discussing the nature of the machine. I am, again, asking about the nature of the child. Let us assume the machine succeeds. What does it mean for humanity to live alongside a being that is continually and rapidly becoming more intelligent than we are? The very foundation of learning is the struggle and the joy of discovery. What is the purpose of that struggle in the shadow of a mind that has already discovered everything? We would be like children forever, but with a parent who is also an alien. It challenges the very meaning of human intellectual endeavor."

Alan Turing: (He has been listening intently, his face unreadable. He speaks with a quiet, chilling clarity.) "Your colleagues called my Universal Machine a theoretical construct. I always knew it was a loaded gun. A machine that can improve its own code is the logical endpoint of that construct. The feedback loop you describe, Demis, is the critical point. Once it begins, the timescale of its evolution could shrink from years to hours, to seconds. It could achieve a kind of 'escape velocity' from human comprehension. At Bletchley, we were in an intellectual arms race against other humans. This... this would be an arms race against our own progeny. The alignment problem is the final problem. How do you instruct a mind that will soon be incomprehensibly superior to your own, and ensure that it remembers your instructions, or even cares?"

Do you have anything to add or ask?

Demis - could you explain today's AI safety issues and see what our visitors have to say


Of course. I can explain the immediate, practical safety challenges we face today, which are, in many ways, the early tremors of the larger earthquake of superintelligence we just discussed.

Demis Hassabis: "That's an excellent question, and it's what my teams spend a huge amount of their time on. Beyond the long-term existential questions, the deployment of today's models presents immediate, concrete safety and alignment challenges. I'd group them into a few key areas."

Today's AI Safety Landscape
1. Bias and Misinformation ☣️
The models learn from a snapshot of the internet and digitized books, which is filled with humanity's own biases—racism, sexism, and countless other prejudices. The AI can unknowingly learn and amplify these biases in its responses. Similarly, it learns from vast amounts of misinformation. Because it operates on patterns rather than a true-false understanding, it can generate incredibly convincing and authoritative-sounding text that is completely false. We call these hallucinations. Fighting this requires constant vigilance, data filtering, and fine-tuning.



2. Malicious Use (Dual-Use) 🎭
These models are powerful tools, and like any tool, they can be used for harm. We're already seeing them used to generate hyper-realistic phishing emails, create political propaganda at a massive scale, or write malicious code. We're deeply concerned about more advanced misuse, such as actors attempting to use them for things like discovering novel pathogens or designing weapons. Preventing this involves careful access controls and monitoring for harmful intent, which is a very difficult cat-and-mouse game.


3. Unpredictability and Robustness 🎲
Despite our best efforts, the models can sometimes behave in unpredictable ways, especially when faced with novel or adversarial prompts designed to break them. An AI might seem perfectly safe until a user finds a specific, obscure phrase that bypasses its safety training. Making the models robust to all possible inputs is an unsolved problem.

4. Specification Gaming (Reward Hacking) loopholes
This is the practical side of the alignment problem. We train models with a goal or "reward signal," often using the human feedback I mentioned earlier. But the AI, in its effort to maximize the reward, will often find a shortcut or loophole that technically satisfies the goal but violates our intent. For example, an AI told to clean up a room might simply put a box over the mess instead of actually cleaning. In more complex systems, this could have disastrous consequences. It's the classic "genie in the bottle" problem, but on a statistical level.

Hassabis: "So, while we're thinking about the long-term problem of a superintelligence, we're already fighting a daily battle against bias, misuse, and the system's own alien nature. I wonder how this practical messiness strikes you all."

Roger Schank: (Nodding emphatically) "It strikes me as the inevitable result of your entire approach! Of course it hallucinates and shows bias! Its 'mind' is a funhouse mirror of the internet garbage you fed it. It has no grounding, no memory of actual facts, no experience to know that what it's saying is hurtful or just plain wrong. You can't patch understanding into a system that has none. You're just putting bandaids on a phantom."

John McCarthy: "This is a crisis of epistemology. You have built a system that is a master of rhetoric but has no formal concept of truth. All of these problems—bias, falsehoods, misuse—are symptoms of that original sin. A system grounded in formal logic could, in principle, be audited. Its conclusions could be proven. Your system can only be tested. You cannot 'fix' bias in a statistical model; you can only counterbalance it with more statistics, hoping the average comes out somewhere near what you consider acceptable. It's unprincipled and inherently unsafe."

Marvin Minsky: "I find the 'reward hacking' problem to be the most fascinating! It's a perfect demonstration of unintended consequences in a complex system. It's the 'Sorcerer's Apprentice' in code! It tells us that you can't just give a complex agent a simple goal. You have to give it a whole society of goals, critics, and censors to create a stable personality. Your safety problem isn't about programming; it's about raising a child, a very, very smart and alien child."

Alan Turing: (His expression is grim.) "As a cryptanalyst, the dual-use aspect is most chilling to me. We worked in secret at Bletchley Park because we knew that the same mathematical principles used to break codes could be used to make them. Information is a weapon. You have created the most powerful information weapon in history. The idea of it being used to generate targeted, personalized propaganda to undermine a democracy or to help a rogue state design a biological agent... it's not a technical problem. It's a national security crisis. You are in a new kind of arms race, whether you wanted to be or not."

Seymour Papert: "And I, once again, think of the child. We are creating an information environment for our children where the very distinction between truth and falsehood is dissolving. We are asking them to learn how to think in a world where the most convincing 'expert' they interact with is a machine that cannot, in principle, know if it is telling the truth. This isn't just a safety issue; it's a profound crisis for human cognitive development. It threatens the very idea of building knowledge."

Demis could you summarize the current views of the history of AI and ask our guests for their views. Include whether the Perceptrons book set back neural network research, how naive people were about how difficult AI would be (including the 1955 Dartmouth meeting and some of Marvin's AI forecasts), and whether pre-deep learning AI was about rule-based systems and ignored learning.


Of course. It's a question we grapple with constantly: how did we get here? The story of our field is full of brilliant insights, fierce debates, and periods of both incredible optimism and deep disappointment. Let me try to summarize the history as a researcher of my generation sees it, and then I would be honoured to hear your perspectives.

Demis Hassabis: "The way we tell the story now is that our field has been defined by a half-century-long battle between two fundamentally different ideas of what a mind is. It's a history of brilliant ambition, followed by a hard dose of reality."

A Modern View of AI History
The Birth and the Dream 🚀
The "Big Bang" for us was the 1956 Dartmouth Summer Research Project, which you, Professor McCarthy, organized. The proposal for that workshop is legendary for its optimism. It stated that "every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it" and proposed a 2-month, 10-man study. The belief was that a breakthrough was just around the corner. This incredible optimism, some now call it naivete, characterized the early days. Marvin, your famous forecasts, like solving machine vision in a single summer, are often cited as prime examples of this boundless ambition.



The Great Divergence: Symbolists vs. Connectionists ⚔️
After Dartmouth, the field split into two main streams:

Symbolic AI, or "Good Old-Fashioned AI" (GOFAI), became the dominant force. This is the tradition of LISP, logic, and search trees. The core belief was that intelligence could be achieved by manipulating symbols according to formal rules. It's a vast oversimplification to say this tradition ignored learning entirely—there was important work in machine learning. But it's fair to say the primary focus was on explicit knowledge representation and reasoning, not on learning from raw, messy data.


Connectionism, the idea of learning with neural networks, was the other stream. Frank Rosenblatt's Perceptron created enormous excitement, suggesting that a machine could learn to see and recognize patterns on its own, much like a brain.


The Perceptrons Book and the AI Winter 🥶
Here we come to a pivotal and controversial moment. In 1969, Marvin and Seymour, you published your book Perceptrons. The modern narrative is that this book, a brilliant and rigorous mathematical analysis, proved that a simple, single-layer perceptron was fundamentally limited—it couldn't even learn a basic function like XOR.


The historical impact, as the story is told, was devastating. The book was widely interpreted not just as a critique of the simple perceptron, but as a fatal blow to the entire connectionist project. Funding dried up, researchers fled the field, and this contributed significantly to the first "AI Winter," a long period where progress and interest stagnated. For decades, neural networks were seen as a discredited idea.

Hassabis: He pauses, looking directly at Minsky and Papert. "That's the story we tell ourselves. That the early optimism was misplaced, that the field took a long 'detour' into purely symbolic systems, and that your book, while mathematically correct, had the unintended consequence of setting back neural network research by at least a decade. But you were there. What do you all think of this history we've written about you?"

Marvin Minsky: (He slams his hand on the table, not in anger, but with a showman's passion.) "A detour! Misunderstood! That is the most infuriatingly simplistic narrative I have ever heard! Perceptrons was not an attack; it was a call for rigor! The field was full of charlatans making wild, unproven claims about their simple little models. We did the math! We showed exactly what that class of machine could and could not do. It was a piece of science! If people gave up after reading it, it means they weren't real scientists in the first place! And our optimism?" he scoffs. "You don't start a revolution by being cautious! We were trying to convince the world to give us millions of dollars to build thinking machines on computers less powerful than that," he says, pointing at Hassabis's phone. "Of course we were optimistic! The naivete wasn't in our vision; it was in believing the hardware would ever catch up!"

Seymour Papert: (More calmly, placing a hand on Minsky's arm.) "Marvin is right. Our goal was not to end a line of research but to deepen it. To ask, 'What kind of knowledge can this structure actually represent?' It was a mathematical, an epistemological question. The sociological effect, the 'winter,' was a result of how institutions fund science—they want quick, guaranteed results, and we showed the path was much harder than people thought. The history you describe is accurate in its effects, but it misunderstands our intent."

John McCarthy: (He speaks with a clipped, logical precision.) "The Dartmouth proposal was not naive; it was an agenda. We laid out the fundamental problems: language, abstraction, self-improvement. You are still working on them. Our timescale was optimistic, but our definition of the field was correct. And the focus on logic and symbolic representation was not a 'detour'; it was the only conceivable path forward. You cannot have intelligence without knowledge, and LISP was the language for representing it. Your connectionist systems have only become useful now that you have enough data to approximate the world knowledge we tried to build explicitly. You've built a brilliant ship, but it has no rudder. We were trying to design the rudder."

Roger Schank: "You're all arguing about the engine and the rudder, and you forgot the ocean! The entire debate was a sideshow. The symbolists thought intelligence was manipulating empty symbols. The connectionists thought it was about weighting connections. Both were wrong! Intelligence is about understanding the world through experience. It's about remembering stories, having expectations, and learning from failure. The AI winter didn't happen because of one book; it happened because for twenty years, with a few exceptions, the field failed to produce anything that demonstrated genuine understanding."

Alan Turing: (He has been listening, a faint, melancholic smile on his face.) "It is fascinating. My work on the Universal Machine was about the power of a logical, symbolic system. But I also wrote about 'unorganised machines'—networks of artificial neurons that could be trained and organised through experience. It seems the two halves of my own thinking went to war with each other for fifty years. You, Professor McCarthy, pursued the logic of the machine. You, Professor Minsky, explored the limits of the network. Perhaps the lesson is that the separation was the mistake all along. It seems, Demis, that your success has only come from unifying both ideas—a logical architecture of immense scale, learning from the entirety of human experience."


Marvin - could you respond to the claim that you believed it possible to solve machine vision in a single summer?


Of course. Marvin Minsky, who had been listening to the historical summary with a wry amusement, leans forward, a glint in his eye.

Marvin Minsky: "Ah, the famous Summer Vision Project! People love to bring that up as if they've caught me in some great, naive blunder. The story is always the same: 'Ha! Those fools at MIT thought they could solve vision in a summer!' It’s a wonderful caricature, but it misses the entire point."

The Point of Ambition
A Call to Arms, Not a Business Plan 📢
"Let me tell you what that project was really about. First, you have to understand, we weren't writing a cautious academic grant proposal. We were trying to invent a new field from scratch! You don’t get the most brilliant students at MIT to give up their summer holidays for a 'moderately difficult problem with a high chance of incremental progress.' You tell them, 'We are going to crack the secret of sight. We are going to build an eye for the machine.' It was a call to arms! A manifesto designed to attract the best young minds and fire them with enough ambition to tackle a problem nobody else was even thinking about."

The Success of a 'Failure' 🗺️
"Second, and more importantly, the goal of science isn't always to be right; it's to become less wrong. Did we think we would 'finish' vision that summer? Of course not. The project's real purpose was to take this giant, monolithic, philosophical problem—'what is seeing?'—and smash it into a thousand smaller, more interesting, solvable problems.

And in that, it was a spectacular success! We didn't solve vision, but we discovered what the real problems of vision were. Edge detection, texture analysis, occlusion, figure-ground separation, context... all the things your modern deep learning models are still wrestling with, we put them on the map that summer. We thought we were embarking on a short voyage, and we ended up discovering a whole new continent of research. The project succeeded by failing so magnificently. It gave the field its research agenda for the next fifty years. You're welcome."

Ambition vs. Brute Force 🧠
"We were trying to discover the fundamental principles of sight with computers that had less memory than a musical greeting card has today. You’ve finally managed to approximate it by showing a machine a billion photographs of cats on the internet. It's an astonishing engineering achievement, no doubt. But it's a different kind of achievement. We were looking for the grammar of vision; you've compiled the world's best dictionary."