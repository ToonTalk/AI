{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f72cf1a",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Learning for Finding Paraphrases of Text\n",
    "\n",
    "This notebook demonstrates how to use machine learning, specifically NLP models, to find paraphrases of given text. We will use the Hugging Face Transformers library, which provides a wide range of pre-trained models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c9969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uncomment and run this cell in Google Colab to install the transformers library\n",
    "# !pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e4d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddf43dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = 't5-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b92e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def paraphrase(text, num_return_sequences=3):\n",
    "    \"\"\"\n",
    "    This function takes a piece of text and generates its paraphrases.\n",
    "    :param text: str - The text to be paraphrased.\n",
    "    :param num_return_sequences: int - The number of paraphrase alternatives to generate.\n",
    "    :return: list - A list of paraphrased sentences.\n",
    "    \"\"\"\n",
    "    # Preprocess the text\n",
    "    input_text = f'paraphrase: {text} </s>'\n",
    "    encoding = tokenizer.encode_plus(input_text, padding=True, return_tensors='pt')\n",
    "\n",
    "    # Generate paraphrases with do_sample set to True\n",
    "    outputs = model.generate(**encoding, max_length=50, num_return_sequences=num_return_sequences, num_beams=5, temperature=1.5, do_sample=True)\n",
    "    \n",
    "    # Decode and return the paraphrases\n",
    "    paraphrases = [tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True) for output in outputs]\n",
    "    return paraphrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a45d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "original_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "paraphrased_texts = paraphrase(original_text)\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(original_text)\n",
    "print(\"\\nParaphrased Texts:\")\n",
    "for i, paraphrase in enumerate(paraphrased_texts, 1):\n",
    "    print(f\"{i}. {paraphrase}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
