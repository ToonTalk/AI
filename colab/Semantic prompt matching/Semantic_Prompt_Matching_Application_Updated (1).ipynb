{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0ed66060",
      "metadata": {
        "id": "0ed66060"
      },
      "source": [
        "\n",
        "# Semantic Prompt Matching Application\n",
        "\n",
        "This notebook guides you through the creation of a simple program that matches a new user prompt to the semantically closest pre-defined prompt and returns the associated response. This is useful for building basic chatbots or automated Q&A systems.\n",
        "\n",
        "## Steps Involved:\n",
        "\n",
        "1. **Data Preparation**: Define a list of pre-defined prompts and their associated responses.\n",
        "2. **Semantic Similarity Calculation**: Implement a method to calculate the semantic similarity between the new prompt and each pre-defined prompt.\n",
        "3. **Finding the Closest Match**: Determine which pre-defined prompt is semantically closest to the new prompt.\n",
        "4. **Returning the Response**: Return the response associated with the closest pre-defined prompt.\n",
        "\n",
        "Let's get started!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c87c73f",
      "metadata": {
        "id": "6c87c73f"
      },
      "source": [
        "\n",
        "## 1. Data Preparation\n",
        "\n",
        "First, we need to prepare our dataset. This includes defining a set of pre-defined prompts and their corresponding responses. We'll store these in a Python dictionary for easy access.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bfe3f7c1",
      "metadata": {
        "id": "bfe3f7c1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Sample data: pre-defined prompts and responses\n",
        "predefined_prompts = {\n",
        "    \"What is your name?\": \"I'm a chatbot created to help you.\",\n",
        "    \"How are you?\": \"I'm just a program, so I don't have feelings, but thanks for asking!\",\n",
        "    \"Tell me a joke.\": \"Why don't scientists trust atoms? Because they make up everything!\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95df5cfc",
      "metadata": {
        "id": "95df5cfc"
      },
      "source": [
        "\n",
        "## 2. Semantic Similarity Calculation (Updated)\n",
        "\n",
        "Instead of using TF-IDF, we will now use a more advanced method involving machine learning embeddings for better semantic understanding. We'll use the Universal Sentence Encoder from TensorFlow Hub to convert sentences into embeddings and then calculate cosine similarity.\n",
        "\n",
        "First, ensure you have TensorFlow and TensorFlow Hub installed:\n",
        "```python\n",
        "!pip install tensorflow tensorflow-hub\n",
        "```\n",
        "\n",
        "Then, load the Universal Sentence Encoder model:\n",
        "```python\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Load the model\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "```\n",
        "\n",
        "Now, we'll update our function to calculate semantic similarity using these embeddings:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aef2aecb",
      "metadata": {
        "id": "aef2aecb"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "\n",
        "# Load the Universal Sentence Encoder\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "# Function to calculate semantic similarity using embeddings\n",
        "def calculate_similarity(new_prompt, predefined_prompts):\n",
        "    # Combine new prompt with predefined prompts\n",
        "    prompts = list(predefined_prompts.keys()) + [new_prompt]\n",
        "\n",
        "    # Generate embeddings for each prompt\n",
        "    embeddings = embed(prompts)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarities = np.inner(embeddings[-1], embeddings[:-1])\n",
        "    return similarities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d707a7ed",
      "metadata": {
        "id": "d707a7ed"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to calculate semantic similarity using embeddings\n",
        "def calculate_similarity(new_prompt, predefined_prompts):\n",
        "    # Combine new prompt with predefined prompts\n",
        "    prompts = list(predefined_prompts.keys()) + [new_prompt]\n",
        "\n",
        "    # Generate embeddings for each prompt\n",
        "    embeddings = embed(prompts)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarities = np.inner(embeddings[-1], embeddings[:-1])\n",
        "    return similarities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae0f7d3d",
      "metadata": {
        "id": "ae0f7d3d"
      },
      "source": [
        "\n",
        "## 3. Finding the Closest Match\n",
        "\n",
        "After calculating the similarities, we need to find the pre-defined prompt that is most similar to the new prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ce1739e0",
      "metadata": {
        "id": "ce1739e0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to find the closest match\n",
        "def find_closest_match(new_prompt, predefined_prompts):\n",
        "    similarities = calculate_similarity(new_prompt, predefined_prompts)\n",
        "    closest_index = similarities.argmax()\n",
        "    closest_prompt = list(predefined_prompts.keys())[closest_index]\n",
        "    return closest_prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e7f96b3",
      "metadata": {
        "id": "0e7f96b3"
      },
      "source": [
        "\n",
        "## 4. Returning the Response\n",
        "\n",
        "Finally, we return the response associated with the closest pre-defined prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3964cac1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3964cac1",
        "outputId": "1c6a1a31-7e7c-41f8-ac51-a1ff3228eefa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Why don't scientists trust atoms? Because they make up everything!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Function to get response for a new prompt\n",
        "def get_response(new_prompt, predefined_prompts):\n",
        "    closest_prompt = find_closest_match(new_prompt, predefined_prompts)\n",
        "    return predefined_prompts[closest_prompt]\n",
        "\n",
        "# Example\n",
        "new_prompt = \"Tell me something funny.\"\n",
        "response = get_response(new_prompt, predefined_prompts)\n",
        "print(\"Response:\", response)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_prompt = \"What do people call you\"\n",
        "response = get_response(new_prompt, predefined_prompts)\n",
        "print(\"Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqEVvG3knySs",
        "outputId": "97a2ff29-36bc-43f3-b25a-b0398ec270bb"
      },
      "id": "iqEVvG3knySs",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: I'm a chatbot created to help you.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}