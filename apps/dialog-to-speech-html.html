<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dialog to Speech Converter</title>
    <style>
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
        }
        body {
            background-color: #f7f9fc;
            color: #333;
            line-height: 1.6;
        }
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
        }
        h1 {
            text-align: center;
            margin-bottom: 24px;
            color: #2c3e50;
        }
        h2 {
            margin-top: 20px;
            margin-bottom: 10px;
            color: #34495e;
        }
        .card {
            background: white;
            border-radius: 8px;
            padding: 24px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            margin-bottom: 24px;
        }
        label {
            display: block;
            margin-bottom: 8px;
            font-weight: 500;
        }
        input, textarea, select {
            width: 100%;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 4px;
            font-size: 16px;
            margin-bottom: 8px;
        }
        input:focus, textarea:focus, select:focus {
            outline: none;
            border-color: #3498db;
            box-shadow: 0 0 0 2px rgba(52, 152, 219, 0.2);
        }
        textarea {
            min-height: 120px;
            resize: vertical;
        }
        .hint {
            font-size: 12px;
            color: #7f8c8d;
            margin-bottom: 16px;
        }
        .button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 16px;
            font-weight: 500;
            display: inline-block;
            text-align: center;
            transition: background-color 0.2s;
        }
        .button:hover {
            background-color: #2980b9;
        }
        .button:disabled {
            background-color: #95a5a6;
            cursor: not-allowed;
        }
        .button-green {
            background-color: #2ecc71;
        }
        .button-green:hover {
            background-color: #27ae60;
        }
        .button-container {
            text-align: center;
            margin: 16px 0;
        }
        .alert {
            background-color: #f8d7da;
            color: #721c24;
            padding: 12px;
            border-radius: 4px;
            margin-bottom: 16px;
        }
        .character-card {
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 16px;
            margin-bottom: 16px;
        }
        .character-name {
            font-weight: 600;
            margin-bottom: 8px;
        }
        .tabs {
            display: flex;
            margin-bottom: 16px;
        }
        .tab {
            padding: 8px 16px;
            cursor: pointer;
            border-bottom: 2px solid transparent;
        }
        .tab.active {
            border-bottom: 2px solid #3498db;
            font-weight: 500;
        }
        .tab-content {
            display: none;
        }
        .tab-content.active {
            display: block;
        }
        .progress-container {
            width: 100%;
            background-color: #f1f1f1;
            border-radius: 5px;
            margin-bottom: 10px;
        }
        .progress-bar {
            height: 10px;
            border-radius: 5px;
            background-color: #3498db;
            width: 0%;
            transition: width 0.3s;
        }
        .audio-controls {
            margin-top: 16px;
        }
        .spinner {
            border: 3px solid rgba(0, 0, 0, 0.1);
            border-radius: 50%;
            border-top: 3px solid #3498db;
            width: 20px;
            height: 20px;
            animation: spin 1s linear infinite;
            display: inline-block;
            vertical-align: middle;
            margin-right: 8px;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
        code {
            background: #f8f9fa;
            padding: 2px 4px;
            border-radius: 3px;
            font-family: monospace;
        }
        pre {
            background: #f8f9fa;
            padding: 16px;
            border-radius: 4px;
            overflow-x: auto;
            font-family: monospace;
            margin-bottom: 16px;
        }
        .log-container {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 16px;
            border-radius: 4px;
            font-family: monospace;
            margin: 16px 0;
            max-height: 200px;
            overflow-y: auto;
        }
        .log-entry {
            margin-bottom: 4px;
        }
        .download-btn {
            background-color: #e74c3c;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 16px;
            font-weight: 500;
            display: inline-block;
            text-align: center;
            transition: background-color 0.2s;
            margin-left: 8px;
        }
        .download-btn:hover {
            background-color: #c0392b;
        }
        .combined-audio-container {
            padding: 16px;
            margin-bottom: 24px;
            background-color: #f5f9ff;
            border: 1px solid #3498db;
            border-radius: 8px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Dialog to Speech Converter</h1>
        
        <div class="card">
            <h2>1. Enter your OpenAI API Key</h2>
            <label for="api-key">OpenAI API Key</label>
            <input type="password" id="api-key" placeholder="Enter your OpenAI API key">
            <p class="hint">Your API key is used only to make direct requests to OpenAI's API and is never sent to any server. It will be used for both generating voice descriptions and creating speech audio.</p>
        </div>
        
        <div class="card">
            <h2>2. Enter Dialog Text</h2>
            <label for="dialog-text">Dialog Text</label>
            <textarea id="dialog-text" placeholder="John: Hello there, how are you today?&#10;Mary: I'm doing great, thanks for asking!"></textarea>
            <p class="hint">Format: Character: Dialog text</p>
            
            <div class="button-container">
                <button id="parse-button" class="button">Parse Dialog</button>
            </div>
        </div>
        
        <div id="characters-container" style="display: none;" class="card">
            <h2>3. Character Voice Settings</h2>
            <div id="characters-list"></div>
        </div>
        
        <div id="generate-container" style="display: none;" class="button-container">
            <button id="generate-button" class="button button-green">Generate Audio</button>
        </div>
        
        <div id="progress-container" style="display: none;" class="card">
            <h2>Generating Audio</h2>
            <p id="progress-text">Processing characters...</p>
            <div class="progress-container">
                <div id="progress-bar" class="progress-bar"></div>
            </div>
            <div id="log-container" class="log-container"></div>
            <p class="hint">The application uses OpenAI's chat completions API to generate unique voice descriptions for each character, then uses those descriptions with the speech API to create distinct voices.</p>
        </div>
        
        <div id="result-container" style="display: none;" class="card">
            <h2>Generated Dialog Audio</h2>
            <div id="audio-container" class="audio-controls"></div>
            
            <div class="button-container">
                <button id="download-button" class="download-btn">Download Audio</button>
            </div>
        </div>
        
        <div id="error-container" style="display: none;" class="alert"></div>
    </div>

    <script>
        // Available voices from OpenAI
        const availableVoices = ['alloy', 'ash', 'ballad', 'coral', 'echo', 'fable', 'onyx', 'nova', 'sage', 'shimmer'];
        
        // Store parsed characters
        let characters = [];
        
        // Store audio segments
        let audioSegments = [];
        
        // Store final audio
        let finalAudio = null;
        
        // Get DOM elements
        const apiKeyInput = document.getElementById('api-key');
        const dialogTextInput = document.getElementById('dialog-text');
        const parseButton = document.getElementById('parse-button');
        const charactersContainer = document.getElementById('characters-container');
        const charactersList = document.getElementById('characters-list');
        const generateContainer = document.getElementById('generate-container');
        const generateButton = document.getElementById('generate-button');
        const progressContainer = document.getElementById('progress-container');
        const progressBar = document.getElementById('progress-bar');
        const progressText = document.getElementById('progress-text');
        const resultContainer = document.getElementById('result-container');
        const audioContainer = document.getElementById('audio-container');
        const downloadButton = document.getElementById('download-button');
        const errorContainer = document.getElementById('error-container');
        const logContainer = document.getElementById('log-container');
        
        // Helper function to split text into smaller chunks at sentence boundaries
        function splitTextIntoChunks(text, maxLength) {
            if (text.length <= maxLength) {
                return [text];
            }
            
            const chunks = [];
            let currentChunk = '';
            
            // Split by sentences first (periods, question marks, exclamation points)
            const sentences = text.match(/[^.!?]+[.!?]+/g) || [];
            
            // If no sentence breaks found, fall back to splitting by commas
            if (sentences.length === 0) {
                const commaParts = text.split(',');
                for (let part of commaParts) {
                    part = part.trim();
                    
                    if (currentChunk.length + part.length + 1 <= maxLength) {
                        currentChunk += (currentChunk ? ', ' : '') + part;
                    } else {
                        if (currentChunk) {
                            chunks.push(currentChunk);
                        }
                        
                        // If a single part is too long, split it by spaces
                        if (part.length > maxLength) {
                            const words = part.split(' ');
                            currentChunk = '';
                            
                            for (const word of words) {
                                if (currentChunk.length + word.length + 1 <= maxLength) {
                                    currentChunk += (currentChunk ? ' ' : '') + word;
                                } else {
                                    chunks.push(currentChunk);
                                    currentChunk = word;
                                }
                            }
                        } else {
                            currentChunk = part;
                        }
                    }
                }
                
                if (currentChunk) {
                    chunks.push(currentChunk);
                }
                
                return chunks;
            }
            
            // Process sentences
            for (const sentence of sentences) {
                if (currentChunk.length + sentence.length <= maxLength) {
                    currentChunk += sentence;
                } else {
                    // If current chunk is not empty, push it
                    if (currentChunk) {
                        chunks.push(currentChunk);
                    }
                    
                    // If a single sentence is too long, split it further
                    if (sentence.length > maxLength) {
                        // Split by commas
                        const commaParts = sentence.split(',');
                        currentChunk = '';
                        
                        for (let part of commaParts) {
                            part = part.trim();
                            
                            if (currentChunk.length + part.length + 1 <= maxLength) {
                                currentChunk += (currentChunk ? ', ' : '') + part;
                            } else {
                                if (currentChunk) {
                                    chunks.push(currentChunk);
                                }
                                
                                // If a single part is still too long, split by spaces
                                if (part.length > maxLength) {
                                    const words = part.split(' ');
                                    currentChunk = '';
                                    
                                    for (const word of words) {
                                        if (currentChunk.length + word.length + 1 <= maxLength) {
                                            currentChunk += (currentChunk ? ' ' : '') + word;
                                        } else {
                                            chunks.push(currentChunk);
                                            currentChunk = word;
                                        }
                                    }
                                } else {
                                    currentChunk = part;
                                }
                            }
                        }
                    } else {
                        currentChunk = sentence;
                    }
                }
            }
            
            // Don't forget the last chunk
            if (currentChunk) {
                chunks.push(currentChunk);
            }
            
            return chunks;
        }

        // Event listeners
        parseButton.addEventListener('click', parseDialog);
        generateButton.addEventListener('click', generateAudio);
        downloadButton.addEventListener('click', downloadAudio);
        
        // Parse dialog text to identify different speakers
        async function parseDialog() {
            const dialogText = dialogTextInput.value.trim();
            
            if (!dialogText) {
                showError('Please enter some dialog text.');
                return;
            }
            
            const apiKey = apiKeyInput.value.trim();
            if (!apiKey) {
                showError('Please enter your OpenAI API key to proceed.');
                return;
            }
            
            // Show progress container for generating voice descriptions
            progressContainer.style.display = 'block';
            progressText.textContent = 'Analyzing dialog and generating voice descriptions...';
            logContainer.innerHTML = '';
            addLog('Starting dialog analysis...');
            
            const lines = dialogText.split('\n').filter(line => line.trim() !== '');
            const speakerRegex = /^([^:]+):\s*(.+)$/;
            
            const parsedCharacters = new Map();
            const parsedLines = [];
            
            let isValid = true;
            
            // First pass: identify all unique characters and their lines
            lines.forEach((line, index) => {
                const match = line.match(speakerRegex);
                if (match) {
                    const [, speaker, text] = match;
                    const trimmedSpeaker = speaker.trim();
                    
                    parsedLines.push({
                        speaker: trimmedSpeaker,
                        text: text.trim()
                    });
                    
                    if (!parsedCharacters.has(trimmedSpeaker)) {
                        // Assign a random voice to each unique character
                        const randomVoice = availableVoices[Math.floor(Math.random() * availableVoices.length)];
                        
                        parsedCharacters.set(trimmedSpeaker, {
                            name: trimmedSpeaker,
                            voice: randomVoice,
                            voiceDescription: getDefaultVoiceDescription() // Temporary placeholder
                        });
                    }
                } else {
                    isValid = false;
                    showError(`Line ${index + 1} does not follow the format "Character: Dialog text".`);
                    progressContainer.style.display = 'none';
                    return;
                }
            });
            
            if (!isValid) return;
            
            // Second pass: generate voice descriptions for each character
            const uniqueCharacters = Array.from(parsedCharacters.keys());
            const totalCharacters = uniqueCharacters.length;
            
            addLog(`Found ${totalCharacters} unique characters in the dialog.`);
            
            try {
                for (let i = 0; i < uniqueCharacters.length; i++) {
                    const character = uniqueCharacters[i];
                    progressText.textContent = `Generating voice for character ${i + 1} of ${totalCharacters}: ${character}`;
                    
                    // Set progress bar
                    const progressPercentage = (i / totalCharacters) * 100;
                    progressBar.style.width = `${progressPercentage}%`;
                    
                    // Generate voice description
                    const voiceDescription = await generateVoiceDescription(character);
                    
                    // Update character data
                    parsedCharacters.get(character).voiceDescription = voiceDescription;
                }
                
                // Complete progress bar
                progressBar.style.width = '100%';
                
                // Store parsed characters
                characters = Array.from(parsedCharacters.values());
                
                // Store parsed lines
                window.parsedLines = parsedLines;
                
                // Display character settings
                displayCharacterSettings();
                
                // Show generate button
                generateContainer.style.display = 'block';
                
                // Hide error
                errorContainer.style.display = 'none';
                
                // Update progress text
                progressText.textContent = 'Voice descriptions generated successfully!';
                addLog('✓ All voice descriptions generated successfully.');
                
            } catch (error) {
                showError(`Error generating voice descriptions: ${error.message}`);
                addLog(`✗ Error: ${error.message}`);
            }
        }
        
        // Generate a voice description for a character using OpenAI's chat completion API
        async function generateVoiceDescription(character) {
            const apiKey = apiKeyInput.value.trim();
            
            if (!apiKey) {
                showError('Please enter your OpenAI API key to generate voice descriptions.');
                return getDefaultVoiceDescription();
            }
            
            try {
                addLog(`Generating distinctive voice description for character: ${character}...`);
                
                // Call OpenAI chat completions API
                const response = await fetch('https://api.openai.com/v1/chat/completions', {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        model: "gpt-4o",
                        messages: [
                            {
                                role: "system",
                                content: "You are a voice director creating specific voice instructions for text-to-speech systems. Create distinctive voice descriptions for different characters based on their name or role."
                            },
                            {
                                role: "user",
                                content: `Create a distinctive voice description for a character named "${character}" in a dialog. The description should include details about voice affect, tone, pacing, emotion, pronunciation, and pauses. Format it with these six categories on separate lines. Make the voice fit the character name's implied personality.`
                            }
                        ]
                    })
                });
                
                if (!response.ok) {
                    const errorData = await response.json();
                    throw new Error(`OpenAI API error: ${errorData.error?.message || response.statusText}`);
                }
                
                const data = await response.json();
                const voiceDescription = data.choices[0].message.content;
                
                addLog(`✓ Voice description generated for ${character}`);
                return voiceDescription;
            } catch (error) {
                addLog(`✗ Error generating voice description for ${character}: ${error.message}`);
                return getDefaultVoiceDescription();
            }
        }
        
        // Default voice description as fallback
        function getDefaultVoiceDescription() {
            return `Voice Affect: Calm and natural; project appropriate emotion and confidence.
Tone: Sincere and engaging—express genuine emotion while maintaining clarity.
Pacing: Steady and moderate; clear enough to be understood easily.
Emotion: Natural emotional range appropriate to context; speak with appropriate warmth or seriousness.
Pronunciation: Clear and precise, emphasizing key words to enhance understanding.
Pauses: Natural pauses to enhance comprehension and emotional impact.`;
        }
        
        // Display character settings
        function displayCharacterSettings() {
            charactersList.innerHTML = '';
            
            characters.forEach((character, index) => {
                const characterCard = document.createElement('div');
                characterCard.className = 'character-card';
                
                const nameElement = document.createElement('div');
                nameElement.className = 'character-name';
                nameElement.textContent = character.name;
                
                const voiceLabel = document.createElement('label');
                voiceLabel.textContent = 'Voice';
                voiceLabel.htmlFor = `voice-${index}`;
                
                const voiceSelect = document.createElement('select');
                voiceSelect.id = `voice-${index}`;
                
                availableVoices.forEach(voice => {
                    const option = document.createElement('option');
                    option.value = voice;
                    option.textContent = voice;
                    if (voice === character.voice) {
                        option.selected = true;
                    }
                    voiceSelect.appendChild(option);
                });
                
                voiceSelect.addEventListener('change', (e) => {
                    characters[index].voice = e.target.value;
                });
                
                const descriptionLabel = document.createElement('label');
                descriptionLabel.textContent = 'Voice Description/Instructions';
                descriptionLabel.htmlFor = `description-${index}`;
                
                const descriptionTextarea = document.createElement('textarea');
                descriptionTextarea.id = `description-${index}`;
                descriptionTextarea.value = character.voiceDescription;
                
                descriptionTextarea.addEventListener('change', (e) => {
                    characters[index].voiceDescription = e.target.value;
                });
                
                characterCard.appendChild(nameElement);
                characterCard.appendChild(voiceLabel);
                characterCard.appendChild(voiceSelect);
                characterCard.appendChild(descriptionLabel);
                characterCard.appendChild(descriptionTextarea);
                
                charactersList.appendChild(characterCard);
            });
            
            charactersContainer.style.display = 'block';
        }
        
        // Show error message
        function showError(message) {
            errorContainer.textContent = message;
            errorContainer.style.display = 'block';
        }
        
        // Add log entry
        function addLog(message) {
            const logEntry = document.createElement('div');
            logEntry.className = 'log-entry';
            logEntry.textContent = message;
            logContainer.appendChild(logEntry);
            logContainer.scrollTop = logContainer.scrollHeight;
        }
        
        // Generate audio from dialog text using OpenAI TTS API
        async function generateAudio() {
            const apiKey = apiKeyInput.value.trim();
            
            if (!apiKey) {
                showError('Please enter your OpenAI API key.');
                return;
            }
            
            // Reset audio segments
            audioSegments = [];
            
            // Show progress container
            progressContainer.style.display = 'block';
            logContainer.innerHTML = '';
            resultContainer.style.display = 'none';
            
            // Disable generate button
            generateButton.disabled = true;
            
            try {
                // Get parsed lines
                const parsedLines = window.parsedLines;
                
                // Calculate total steps
                const totalSteps = parsedLines.length;
                let completedSteps = 0;
                
                addLog('Starting audio generation process...');
                
                // Create array to hold all audio blobs directly from API responses
                const audioBlobs = [];
                
                // Generate audio for each line
                for (let i = 0; i < parsedLines.length; i++) {
                    const line = parsedLines[i];
                    const character = characters.find(c => c.name === line.speaker);
                    
                    progressText.textContent = `Processing line ${i + 1} of ${parsedLines.length} (${character.name})`;
                    addLog(`Generating speech for ${character.name}: "${line.text.substring(0, 30)}${line.text.length > 30 ? '...' : ''}"`);
                    
                    try {
                        // Split long text into chunks to avoid API limits
                        const MAX_CHUNK_LENGTH = 300; // Much smaller chunks
                        const textChunks = splitTextIntoChunks(line.text, MAX_CHUNK_LENGTH);
                        
                        addLog(`Text for ${character.name} split into ${textChunks.length} chunks for processing`);
                        
                        // Process each chunk separately and collect the blobs
                        const chunkBlobs = [];
                        for (let j = 0; j < textChunks.length; j++) {
                            const chunk = textChunks[j];
                            addLog(`Processing chunk ${j+1}/${textChunks.length} for ${character.name} (${chunk.length} chars)`);
                            
                            // Try with different formats if needed
                            let attempts = 0;
                            let success = false;
                            let audioBlob = null;
                            const formats = ['mp3', 'wav', 'aac'];
                            
                            while (!success && attempts < formats.length) {
                                try {
                                    const format = formats[attempts];
                                    addLog(`Attempt ${attempts + 1}: Using format ${format}`);
                                    
                                    // Call OpenAI TTS API for this chunk
                                    const response = await fetch('https://api.openai.com/v1/audio/speech', {
                                        method: 'POST',
                                        headers: {
                                            'Authorization': `Bearer ${apiKey}`,
                                            'Content-Type': 'application/json'
                                        },
                                        body: JSON.stringify({
                                            model: 'gpt-4o-mini-tts',
                                            voice: character.voice,
                                            input: chunk,
                                            instructions: character.voiceDescription,
                                            response_format: format
                                        })
                                    });
                                    
                                    if (!response.ok) {
                                        const errorData = await response.json();
                                        throw new Error(`OpenAI API error: ${errorData.error?.message || response.statusText}`);
                                    }
                                    
                                    // Get audio data as blob directly and verify size
                                    audioBlob = await response.blob();
                                    
                                    // Verify blob size is reasonable (not truncated)
                                    if (audioBlob.size < 100) {
                                        throw new Error(`Suspiciously small audio file (${audioBlob.size} bytes)`);
                                    }
                                    
                                    addLog(`✓ Successfully generated audio chunk ${j+1} as ${format} (${audioBlob.size} bytes)`);
                                    success = true;
                                } catch (error) {
                                    addLog(`✗ Attempt ${attempts + 1} failed: ${error.message}`);
                                    attempts++;
                                }
                            }
                            
                            if (!success) {
                                addLog(`⚠️ Failed to generate audio for chunk after ${attempts} attempts. Skipping to next chunk.`);
                                // Create a silent audio blob as placeholder
                                audioBlob = await createSilentAudio(0.5);
                            }
                            
                            chunkBlobs.push(audioBlob);
                        }
                        
                        // Store segment info
                        audioBlobs.push({
                            speaker: character.name,
                            blobs: chunkBlobs,
                            text: line.text
                        });
                        
                        addLog(`✓ Successfully generated complete audio for ${character.name}`);
                        
                        // Update progress
                        completedSteps++;
                        const progressPercentage = (completedSteps / totalSteps) * 100;
                        progressBar.style.width = `${progressPercentage}%`;
                        
                    } catch (error) {
                        addLog(`✗ Error generating audio for ${character.name}: ${error.message}`);
                        throw error;
                    }
                }
                
                // All audio segments generated successfully
                progressText.textContent = 'Creating combined audio file...';
                addLog('All speech segments generated. Creating combined audio file...');
                
                try {
                    // Create combined audio file
                    const combinedAudioBlob = await mergeAudioBlobs(audioBlobs);
                    
                    // Store the combined audio for download
                    finalAudio = combinedAudioBlob;
                    
                    // Create playback interface
                    createPlaybackInterface(combinedAudioBlob);
                    
                    // Show result container
                    resultContainer.style.display = 'block';
                    
                    // Re-enable generate button
                    generateButton.disabled = false;
                    
                    addLog('✓ Audio generation complete!');
                    progressText.textContent = 'Audio generation complete!';
                } catch (error) {
                    showError(`Error creating combined audio: ${error.message}`);
                    addLog(`✗ Error: ${error.message}`);
                    generateButton.disabled = false;
                }
                
            } catch (error) {
                showError(`Error generating audio: ${error.message}`);
                addLog(`✗ Error: ${error.message}`);
                generateButton.disabled = false;
            }
        }
        
        // Function to check if an audio buffer contains actual audio content
        function hasAudioContent(audioBuffer) {
            // Get the first channel data
            const channelData = audioBuffer.getChannelData(0);
            
            // Calculate RMS (Root Mean Square) value to detect audio presence
            let sum = 0;
            let count = 0;
            
            // Sample at most 1000 points throughout the buffer for efficiency
            const step = Math.max(1, Math.floor(channelData.length / 1000));
            
            for (let i = 0; i < channelData.length; i += step) {
                sum += channelData[i] * channelData[i];
                count++;
            }
            
            const rms = Math.sqrt(sum / count);
            
            // If RMS is very small, it's likely silence or background noise
            return rms > 0.001; // Threshold can be adjusted if needed
        }
        
        // Function to merge all audio blobs into a single file
        async function mergeAudioBlobs(audioSegments) {
            addLog('Starting audio merging process...');
            progressText.textContent = 'Merging audio segments...';
            
            // Check if Web Audio API is available
            if (!window.AudioContext && !window.webkitAudioContext) {
                throw new Error('Web Audio API is not supported in this browser.');
            }
            
            // Create audio context
            const AudioContext = window.AudioContext || window.webkitAudioContext;
            const audioContext = new AudioContext();
            
            // Array to store all decoded audio buffers
            const audioBuffers = [];
            
            // Total duration for progress calculation
            let totalDuration = 0;
            
            // Convert each blob to an audio buffer
            for (let i = 0; i < audioSegments.length; i++) {
                const segment = audioSegments[i];
                progressText.textContent = `Decoding audio segment ${i + 1} of ${audioSegments.length}...`;
                
                // Process each chunk in this segment
                for (let j = 0; j < segment.blobs.length; j++) {
                    const blob = segment.blobs[j];
                    
                    // Skip any blobs that are too small (likely empty or corrupted)
                    if (blob.size < 200) {
                        addLog(`⚠️ Skipping suspiciously small blob in segment ${i + 1}, chunk ${j + 1} (${blob.size} bytes)`);
                        continue;
                    }
                    
                    try {
                        // Convert blob to array buffer
                        const arrayBuffer = await blob.arrayBuffer();
                        
                        // Decode audio data
                        const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                        
                        // Only add buffers that actually contain audio (not silence)
                        if (hasAudioContent(audioBuffer)) {
                            // Store audio buffer
                            audioBuffers.push(audioBuffer);
                            
                            // Add to total duration
                            totalDuration += audioBuffer.duration;
                            
                            addLog(`✓ Decoded audio segment ${i + 1}, chunk ${j + 1} (${audioBuffer.duration.toFixed(2)}s)`);
                        } else {
                            addLog(`⚠️ Skipping silent audio in segment ${i + 1}, chunk ${j + 1}`);
                        }
                    } catch (error) {
                        addLog(`✗ Error decoding segment ${i + 1}, chunk ${j + 1}: ${error.message}`);
                        // Don't add a silent buffer here, just skip it
                    }
                }
                
                // Add a short silence between segments (decreased from 0.5s to 0.25s)
                const sampleRate = 44100;
                const silenceBuffer = audioContext.createBuffer(1, sampleRate * 0.25, sampleRate); // 0.25 second silence
                audioBuffers.push(silenceBuffer);
                totalDuration += 0.25; // Add silence duration
            }
            
            // If we have no valid audio buffers, throw an error
            if (audioBuffers.length === 0) {
                throw new Error('No valid audio segments could be decoded');
            }
            
            // Calculate total size of the combined buffer
            let totalLength = 0;
            for (const buffer of audioBuffers) {
                totalLength += buffer.length;
            }
            
            // Create a new buffer with the total length
            const mergedBuffer = audioContext.createBuffer(
                audioBuffers[0].numberOfChannels,
                totalLength,
                audioBuffers[0].sampleRate
            );
            
            // Copy data from each buffer to the merged buffer
            let offset = 0;
            for (const buffer of audioBuffers) {
                for (let channel = 0; channel < buffer.numberOfChannels; channel++) {
                    const channelData = buffer.getChannelData(channel);
                    mergedBuffer.getChannelData(channel).set(channelData, offset);
                }
                offset += buffer.length;
            }
            
            addLog(`✓ Successfully merged all audio segments (${totalDuration.toFixed(2)}s total)`);
            
            // Convert merged buffer to WAV blob
            progressText.textContent = 'Converting to WAV format...';
            const wavBlob = await bufferToWav(mergedBuffer);
            
            addLog(`✓ Created WAV file (${(wavBlob.size / 1024 / 1024).toFixed(2)} MB)`);
            return wavBlob;
        }
        
        // Convert AudioBuffer to WAV Blob
        function bufferToWav(buffer) {
            return new Promise(resolve => {
                const numberOfChannels = buffer.numberOfChannels;
                const sampleRate = buffer.sampleRate;
                const length = buffer.length;
                
                // Create a WAV file format
                const dataView = encodeWAV(buffer);
                const audioBlob = new Blob([dataView], { type: 'audio/wav' });
                
                resolve(audioBlob);
            });
        }
        
        // Function to encode AudioBuffer to WAV format
        function encodeWAV(buffer) {
            const numChannels = buffer.numberOfChannels;
            const sampleRate = buffer.sampleRate;
            const numSamples = buffer.length;
            const bytesPerSample = 2; // 16-bit
            const blockAlign = numChannels * bytesPerSample;
            const byteRate = sampleRate * blockAlign;
            const dataSize = numSamples * blockAlign;
            
            const buffer16 = new Int16Array(numSamples * numChannels);
            
            // Get audio data from each channel
            for (let c = 0; c < numChannels; c++) {
                const channel = buffer.getChannelData(c);
                for (let i = 0; i < numSamples; i++) {
                    // Convert to 16-bit signed integer
                    const s = Math.min(1, Math.max(-1, channel[i]));
                    buffer16[i * numChannels + c] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                }
            }
            
            const wavHeader = new ArrayBuffer(44);
            const view = new DataView(wavHeader);
            
            // RIFF identifier
            writeString(view, 0, 'RIFF');
            // RIFF chunk length
            view.setUint32(4, 36 + dataSize, true);
            // RIFF type
            writeString(view, 8, 'WAVE');
            // format chunk identifier
            writeString(view, 12, 'fmt ');
            // format chunk length
            view.setUint32(16, 16, true);
            // sample format (1 for PCM)
            view.setUint16(20, 1, true);
            // number of channels
            view.setUint16(22, numChannels, true);
            // sample rate
            view.setUint32(24, sampleRate, true);
            // byte rate (sample rate * block align)
            view.setUint32(28, byteRate, true);
            // block align (channel count * bytes per sample)
            view.setUint16(32, blockAlign, true);
            // bits per sample
            view.setUint16(34, 8 * bytesPerSample, true);
            // data chunk identifier
            writeString(view, 36, 'data');
            // data chunk length
            view.setUint32(40, dataSize, true);
            
            // Create the final WAV buffer
            const wav = new Uint8Array(wavHeader.byteLength + buffer16.byteLength);
            wav.set(new Uint8Array(wavHeader), 0);
            wav.set(new Uint8Array(buffer16.buffer), wavHeader.byteLength);
            
            return wav;
        }
        
        // Helper function to write a string to a DataView
        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }
        
        // Create silent audio for fallbacks
        function createSilentAudio(durationSec) {
            return new Promise((resolve) => {
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const sampleRate = audioContext.sampleRate;
                const buffer = audioContext.createBuffer(1, durationSec * sampleRate, sampleRate);
                
                // Create a WAV from the silent buffer
                const wav = encodeWAV(buffer);
                const silentBlob = new Blob([wav], { type: 'audio/wav' });
                resolve(silentBlob);
            });
        }
        
        // Create playback interface for the combined audio
        function createPlaybackInterface(combinedAudioBlob) {
            // Clear audio container
            audioContainer.innerHTML = '';
            
            // Create a container for the combined audio
            const combinedContainer = document.createElement('div');
            combinedContainer.className = 'combined-audio-container';
            
            // Create audio element
            const audioElement = document.createElement('audio');
            audioElement.controls = true;
            audioElement.style.width = '100%';
            audioElement.style.marginTop = '10px';
            
            // Set audio source
            const audioUrl = URL.createObjectURL(combinedAudioBlob);
            audioElement.src = audioUrl;
            
            // Add information text
            const infoText = document.createElement('p');
            infoText.textContent = 'Complete dialog audio with all characters. Use the controls to play or pause.';
            infoText.style.fontSize = '14px';
            infoText.style.marginTop = '10px';
            infoText.style.color = '#555';
            
            // Add to container
            combinedContainer.appendChild(audioElement);
            combinedContainer.appendChild(infoText);
            
            // Add container to main audio container
            audioContainer.appendChild(combinedContainer);
        }
        
        // Download the combined audio file
        function downloadAudio() {
            if (!finalAudio) {
                showError('No audio to download.');
                return;
            }
            
            try {
                // Create download link
                const downloadUrl = URL.createObjectURL(finalAudio);
                const downloadLink = document.createElement('a');
                downloadLink.href = downloadUrl;
                downloadLink.download = 'complete-dialog.wav';
                document.body.appendChild(downloadLink);
                downloadLink.click();
                document.body.removeChild(downloadLink);
                
                addLog('✓ Download complete!');
                progressText.textContent = 'Download complete!';
            } catch (error) {
                showError(`Error preparing download: ${error.message}`);
                addLog(`✗ Error: ${error.message}`);
            }
        }
    </script>
</body>
</html>
