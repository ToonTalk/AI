Automated Generation of
 Illustrated Stories 


Ken Kahn <toontalk@gmail.com>


Note this documentation is about the generator constructed in Snap! Here is the documentation about the reconstruction of this in JavaScript created by collaborating with ChatGPT-4.
Introduction
After watching the Netflix Sandman series, I got the idea of using GPT-3 and text-to-image models to generate illustrated stories. In the Calliope story by Neil Gaiman, a writer is cursed with too many good ideas. They are
1. A train full of silent women plunging forever through the twilight
2. A man who inherits a library card to the library of Alexandria
3. Heads made of light
4. A rosebush, a nightingale, and a black rubber dog collar
5. A city in which the streets are paved with time
6. A small piece of blue cardboard
7. A plum, sweet and tart and cold
8. A were-goldfish who transforms into a wolf at full moon
9. Two old women taking a weasel on a holiday
10. Gryphons  shouldn't marry
11.     Vampires don't dance
12. A man who falls in love with a paper doll
13. The sun setting over the Parthenon
14. Shark's teeth soup
15. An old man in Sunderland who owned the universe, and who kept it in a jam jar in the dusty cupboard under the stairs


You can find these “story ideas” in this copy of Neil Gaiman’s Sandman graphic novel. Also in the audio book and television series. I decided to generate illustrated stories based only upon these story titles. Later I experimented with additional stories.
Generating first drafts
I wondered if I could automate the writing and illustration of these story ideas. To start I simply prepended “Write a story about” to each story idea and asked the largest version of GPT-3 (text-davinci-002) to complete the request. (Temperature was set to 0.7 and the maximum number of generated tokens was 512.) 
Generating successive drafts
I wondered if I could get GPT-3 to improve these “first drafts”. I created the prompt “The following story has been generated by an AI program. It can edit the story if given instructions. Generate a list of 2 instructions tailored to improve this story: <pasted the current version of the story here>”. (This time the maximum number of tokens was 100.)


As I write this OpenAI has a beta version of GPT-3  (text-davinci-edit-001) that can edit text based on provided instructions. I used this to apply the edits that text-davinci-002 suggested to the current version of the story. I iterated this five times. Sometimes a bug in text-davinci-edit-001 caused it to incorporate the text of the instructions instead of following them. And sometimes it duplicated paragraphs. When either of these things occurred (luckily not that often) I used the version just before it happened.


Here is the history of edits of each of the stories:


1. A train full of silent women plunging forever through the twilight
2. A man who inherits a library card to the library of Alexandria
3. Heads made of light
4. A rosebush, a nightingale, and a black rubber dog collar
5. A city in which the streets are paved with time
6. A small piece of blue cardboard
7. A plum, sweet and tart and cold
8. A were-goldfish who transforms into a wolf at full moon
9. Two old women taking a weasel on a holiday
10. Gryphons  shouldn't marry
11. Vampires don't dance
12. A man who falls in love with a paper doll
13. The sun setting over the Parthenon
14. Shark's teeth soup
15. An old man in Sunderland who owned the universe, and who kept it in a jam jar in the dusty cupboard under the stairs
Generating Image prompts
The next challenge was to illustrate the generated stories. There are many text-to-image systems that can be used. I used GPT-3 to generate three image prompts and chose the one I thought most promising. (Perhaps results would have been almost as good with just the generation of one image prompt.) The prompt I used is:


Here are descriptions of illustrations for the text on pages from an illustrated book.
Text: A girl eats the little bear's porridge, breaks his little chair, and falls asleep in his little bed.
Illustration: A girl sleeping in a small bed next to a broken chair and an empty bowl.
Text: Her fairy godmother appears and transforms a pumpkin into a golden coach to take Cinderella to the ball. 
Illustration: A girl is riding in a pumpkin-shaped carriage.
Text:  A hungry grasshopper begs for food from an ant when winter comes and is refused.
Illustration: A hungry grasshopper is begging an ant.
Text: Now when the men observed the earth through the lower window, it looked like nothing more than a dark spot, drowned in the solar rays.[1]
Illustration: Men looking out of a window at the Earth from far away.


The book is about <summary of the current story>
Text: <a paragraph from the story>
Illustration:
Summarizing stories
The summary of each story was generated by GPT-3 with the prompt:


Read the following story:
<text from the current story>
Write a two sentence summary of the story.
Summary:


The reason for including the summary in the prompt for generating image prompts is that often the current paragraph has too little context. For example, the paragraph “He made his way to the club and danced the night away. He had a great time and he was already planning his next escape.” resulted in three identical copies of “A man dancing at a club”. When the summary “A vampire who is tired of cleaning up other vampires' messes and watching over them decides to sneak out and have some fun. When he gets back, the other vampires are angry and make him pay by making him do all the work” was added to the prompt, the image prompt became “A vampire dancing in a club”.


The image prompts are generated for each paragraph of the stories. In one case, the A were-goldfish who transforms into a wolf at full moon story was one long paragraph. I proceeded to both use it and to generate a version where the paragraph was manually broken into three. (In another experiment nearly each sentence was a paragraph and by combining them more reasonable image prompts were generated.)
Generating Illustrations
I then prepended “Artstation. A beautiful oil painting of” to the generated image prompt and submitted it to DALL·E 2 to obtain 4 candidates and to Stable Diffusion (CompVis/stable-diffusion-v1-4 from HuggingFace) to generate 12 candidates. I used my judgment to select among the 16 candidates.
Generating style and mood suggestions
After generating the first eleven illustrated stories it occurred to me that GPT-3 could also be generating style and mood suggestions that could be added to prompts to the text-to-image generators. The prompt for doing this is


We plan to illustrate a story that can be summarized as: <summary>
The paragraph from the story we want to illustrate is: <current paragraph>
The <either ‘mood’, ‘style’, or ‘perspective’> of the illustration could be


Currently only one story was re-illustrated with style and mood: A city in which the streets are paved with time (style and mood added).  The stories A man who falls in love with a paper doll, The sun setting over the Parthenon, Shark's teeth soup, and An old man in Sunderland who owned the universe, and who kept it in a jam jar in the dusty cupboard under the stairs were illustrated with style and mood suggestions. See the presentation notes to see the generated style and mood suggestions. 
Generating perspective suggestions
The same prompt that generates mood and style suggestions can be used to generate “perspectives”. For example, A city in which the streets are paved with time (with perspective, style and mood suggestions) was illustrated using the following generated perspectives:
1. that it could be from the point of view of somebody walking through the city, looking up at the buildings and clock tower.
2. that it is looking up at the tower from the street level, with the buildings towering around it
3. from above looking down, as if the city is a living, breathing entity
The illustrations for A man who falls in love with a paper doll (with perspectives) were also regenerated with perspective suggestions.


I began to notice that sometimes “perspective” is interpreted more abstractly than desired. E.g.,
The perspective is :  From Cinderella's point of view Cinderella's step mother and step sisters make her do hard work and don't allow her to go to the ball.
To address this I changed the prompt to “camera viewpoint” resulting in the intended completions. E.g.
The camera viewport is : Cinderella holding up the glass slipper to the camera, so that we can see that it fits her perfectly.
Generating lighting suggestions
I regenerated the illustrations of A city in which the streets are paved with time (with perspective, style and mood suggestions) with lighting suggestions to produce A city in which the streets are paved with time - added lighting (with perspective, style and mood suggestions). The lighting suggestions generated were:
1. either natural or artificial, but should give the feeling of being late at night
2. dim, with the tower being the brightest object in the image
3. moody and atmospheric, with the person being the only source of light in the image 


I completely regenerated A city in which the streets are paved with time (with perspective, style and mood suggestions)  to produce A city in which the streets are paved with time (with lighting illustration suggestions). The story is completely different since each time the story generator samples the space of possibilities differently. Some of the lighting suggestions generated were:
1. The lighting is bright and harsh, to reflect the fast pace of the city.
2. The lighting is bright and cheerful, to reflect the importance of time in the city…
3. The lighting is : The light shining through the library windows.
Generating art movement suggestions
E.g. Impressionism or realism. Results in some nice variety of images. Here is an example using this new feature using the fully automated version of the app.
Generating medium suggestions
I recreated A city in which the streets are paved with time (with lighting illustration suggestions) replacing the constant medium description (“Artstation. A beautiful oil painting of”) with medium suggestions to produce A city in which the streets are paved with time - added medium and lighting (with perspective, style and mood suggestions). The generated suggestions were:
1. charcoal, to give it a dark and mysterious feel
2. pen and ink, with the tower in the center of the illustration and the streets radiating out from it
3. anything from a traditional painting to a more modern digital illustration
Cleaning up the combined illustration prompt
The prompt is assembled from all these parts. A typical result is
An image of A boy playing with robots in a living room. His sister is sitting nearby with a book.  The medium is : Pencil. The perspective is : The illustration would show Ellis and his sister from a side view. The mood is : The mood of the illustration is playful and sibling-bond-like. The style is : The illustration could have a cartoonish look, with the boy and his sister looking like they're in a cheerful, playful mood. The lighting is : The room is brightly lit from a window.


Applying the GPT-3 edit model to that prompt with the instruction to “Rewrite in clearer language” results in
An image of a boy playing with robots in a living room. His sister is sitting nearby with a book. The medium is a pencil drawing. The perspective is Ellis and his sister from a side view. The mood is playful and sibling-bond-like. The style is cartoonish, with the boy and his sister looking like they’re in a cheerful, playful mood. The room is brightly lit from a window.


After seeing GPT-3 edit (version 001) produce an absurd rewriting (in which an airplane was replaced with a dog - this despite a temperature of zero), I experimented with using the language model instead. Using the following template worked about as well as GPT-3 edit (and works with other language model providers):


We need to rewrite the following in clearer language:
<combined illustration prompt>
A good rewrite is
Some aspects are inconsistent if generated separately
Notice in the following generated prompt that the medium, art movement, and style are incompatible here
The image is of The skyline of Hong Kong, the tram going up to Victoria Peak, and people walking through a traditional Chinese market. The medium is oil paints. The illustration could be done in a traditional Chinese painting style, with bold and bright colors. The illustration would show the skyline of Hong Kong in the background, with the tram going up to Victoria Peak in the foreground, and people walking through the traditional Chinese market in the foreground. The mood is calm. The style is cartoonish/simple. The skyline is illuminated by the sun, the tram is illuminated by the lights of the city, and the people in the market are illuminated by lanterns.


A solution was to generate those aspects with a prompt that describes the previously generated aspects.  E.g art movement is generated based partially on the chosen medium and that the generation of style is influenced by the medium and art movement. A generated prompt for the same part of a story as above is
An image of a busy Hong Kong city street with people walking, driving cars and scooters, and the skyline of Hong Kong in the distance. The medium is vector art. The art movement is graphic novel style. The camera viewport is a wide, aerial view of the bustling city with the skyline in the background. The mood is excitement. The style is graphic novel style, with bright colors, bold shapes and lines, and with dynamic angles and angles of motion to convey the energy and movement of the city. The lighting is bright overhead sunlight to create crisp shadows and vibrant colors.  


Here is a project that displays the “visit to Hong Kong” story using prompt aspects influenced by earlier aspects.
Generating Font Suggestions
Finally, I used the following GPT-3 prompt to generate suggested font choices (0 temperature, 100 maximum tokens):


We need to choose an appropriate font for a story. Suggest several font families that would be best for the following story: <summary of story>
A list of 6 fonts that are available in Google Docs that would work well for the story follows:


Sometimes it would suggest fonts such as “Creepy” which when one googles “creepy font” results in web pages listing several creepy fonts. But I only used fonts that Google Docs supports. (Occasionally the font suggested didn’t exist but a similar one did, e.g., instead of “Gothic”, Google Docs listed “Nanum Gothic” as an option.) I’m not sure if the font suggestions were any good. Perhaps more prompt tuning would help.
Creating Illustrated Stories
I created the final stories in Google Docs Slideshow by copy and paste of generated elements. I used the presenter notes to document some of the generation details. Here are the stories


1. A train full of silent women plunging forever through the twilight
2. A man who inherits a library card to the library of Alexandria
3. Heads made of light
4. A rosebush, a nightingale, and a black rubber dog collar
5. A city in which the streets are paved with time
6. A city in which the streets are paved with time (with style and mood suggestions)
7. A city in which the streets are paved with time (with perspective, style and mood suggestions)
8. A small piece of blue cardboard
9. A plum, sweet and tart and cold
10. A were-goldfish who transforms into a wolf at full moon
11. A were-goldfish who transforms into a wolf at full moon (broken into three paragraphs)
12. Two old women taking a weasel on a holiday
13. Gryphons  shouldn't marry
14. Vampires don't dance
15. A man who falls in love with a paper doll
16. The sun setting over the Parthenon
17. Shark's teeth soup
18. An old mShark's teeth soupan in Sunderland who owned the universe, and who kept it in a jam jar in the dusty cupboard under the stairs
A Snap! Implementation
I am interested in enabling school children to explore this technology. I created custom blocks in Snap! a blocks-based programming language to help others explore illustrated story generation. Non-expert programmers can use these blocks to create first drafts, editing suggestions, performing edits, and generating image prompts. Perhaps such activities could be integrated into English, creative writing, and art classes. There is a great room for creatively exploring the possibilities of co-creation of illustrated stories with AI systems.


This Snap! project can be run in any browser to generate stories and image prompts. It requires that the user has an OpenAI API key. Generating a story and prompts costs a penny or two and new users can obtain a free $18 credit good for 3 months. Note that the API is currently not available in all countries and a VPN may be necessary. 
Several more stories
I gave a talk about this work at the University of Hong Kong and received three story suggestions from the audience which I turned into illustrated stories.
1. A rabbit who travels to another galaxy (I generated this story live)
2. A story of a storywriter who is writing a story
3. A talking star 


I also created this Snap! Project that reads the story that displays the Talking Star story while narrating it. This works well since this story ended up with many one sentence paragraphs.


After some tweaking of the app I generated this A girl named Karen hurt her thumb when a closet door closed. 


Using version 003 of GPT-3 Davinci, I generated this In the 1960s Wally, as a teenager, built a Pietenpol airplane in the basement and In the 1960s Wally, as a teenager, built a Pietenpol airplane in the basement version 2.
Fully automated generation of illustrated stories
When OpenAI released an API to DALLE-2 I experimented with a non-interactive version of this project. Given only the title, it generates a story and illustration prompts for each sentence (previously it was each paragraph). These prompts are sent to DALLE-2 and a narrated slideshow is automatically generated. After generating an illustrated story one can export the data from Snap! to import into a player project. Here are some videos created in this fully automated mode:
1. a rainbow octopus chases a rainbow fish (26 seconds, $0.17 to create)
2. a celebration of Jay's 79th birthday (23 seconds)
3. a boy makes a fire truck with Lego bricks (29 seconds, $0.24 to create)
4. Jack and Jill went up a hill (23 seconds, $0.19 to create)
An interactive projector of illustrated stories
The generator of illustrated stories can produce a data structure that can be displayed by a story projector. Here is a project that displays a story generated from “a visit to Hong Kong” using the projector. If you click on any illustration the prompt that generated it is displayed. Click on the resume button to continue seeing the story. Note that the illustration prompts were (unlike most of the other illustrated stories here) generated with the latest version of GPT-3 (text-davinci-003) and the generation of some aspects of the prompt were influenced by earlier aspect decisions. See this earlier discussion of prompt generation.
Discussion
One way to view large language models like GPT-3 is that they can only do “stream of consciousness” writing. This is a consequence of their training which optimizes a model for determining what the likely next tokens are (tokens are roughly the same as words), given the user’s prompt and the generated text so far. What I’ve done here is to get GPT-3 to review and edit its output by feeding its output back to it as part of a prompt. GPT-3 has no idea that it generated the text - it has no memory. It is not accurate to think of GPT-3 as an entity that generates a story, reflects on it, makes edits, and reflects some more. Instead GPT-3 generates a story, a clone of GPT-3 (a fresh instance) reflects on that story to generate editing instructions, and then a variant of GPT-3 that is “skilled” in editing takes in the story and instructions and produces a revised story.


Obviously the quality of the stories and illustrations varies. While overall I’m very pleased with the results, I should really utilize crowdworkers to evaluate the outputs. I could repeat this experiment several times for each story title to see the range of variation possible. I could experiment with different temperature settings, number of tokens, and prompts. I could use other text-to-image generators.


The only judgment calls I made is selecting among the image prompts and image candidates. One could imagine a process that is more collaborative where several first drafts are proposed and one chooses among them. Perhaps one might edit the current story. One could approve or reject editing suggestions. One could continue generating drafts until there was no more progress.


Ideally the tools I used would be free and easy for all to use via an API. Using the OpenAI API to use GPT-3 to generate a story, editing instructions, font suggestions, and image prompts cost me 7 US cents. Using the DALL·E 2 currently costs 2 cents per image (a little less for lower-resolution images). Stable Diffusion is free. Using it currently requires some technical expertise and  appropriate hardware. Google Colab can run it for free.


There is now a model that can generate videos from text and with an API it could be integrated into this project. One can imagine generating suggestions for what sort of voice should narrate the story and what kind of music should play in the background.


The field of text and image generators is rapidly improving and presumably newer versions will produce higher quality illustrated stories.
Use by children?
I am very curious what projects like this one (and better ones in the future) will mean for children. I suspect that many children will request stories that match their interests. It takes only one or two minutes to generate a 5 or 6 page illustrated story.
Related Work
I’m still investigating. 


Here’s one that is integrated with Google Sheets. 


Here’s one called Prompt Parrot that fine-tunes GPT-2 to generate text-to-image prompts.
________________
[1] From Jules Verne’s From the Earth to the Moon.